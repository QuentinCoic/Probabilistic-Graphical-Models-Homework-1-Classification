{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DM1 Modèles graphiques probabilistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataTrain = np.loadtxt(\"classificationA.train\")\n",
    "dataTest = np.loadtxt(\"classificationA.test\")\n",
    "\n",
    "\n",
    "dataTrainB = np.loadtxt(\"classificationB.train\")\n",
    "dataTestB = np.loadtxt(\"classificationB.test\")\n",
    "\n",
    "dataTrainC = np.loadtxt(\"classificationC.train\")\n",
    "dataTestC = np.loadtxt(\"classificationC.test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear discriminant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LDA(dataTrain, dataTest, ech):\n",
    "    n, p = dataTrain.shape\n",
    "    n2, p2 = dataTest.shape\n",
    "    \n",
    "    mu1 = np.mean(dataTrain[dataTrain[0:,2] == 1][0:,0:2], axis=0) #mu1 chapeau\n",
    "    mu0 = np.mean(dataTrain[dataTrain[0:,2] == 0][0:,0:2], axis=0) #mu0 chapeau\n",
    "\n",
    "    s1 = dataTrain[dataTrain[0:,2] == 1][0:, 0:2] - mu1\n",
    "    sig1 = np.dot(s1.transpose(), s1)\n",
    "\n",
    "    s0 = dataTrain[dataTrain[0:,2] == 0][0:, 0:2] - mu0\n",
    "    sig0 = np.dot(s0.transpose(), s0)\n",
    "\n",
    "    sigma = (sig0 + sig1)/n #Sigma chapeau\n",
    "    invsig = np.linalg.inv(sigma)  #Inverse de sigma chapeau\n",
    "\n",
    "    print(\"Par maximum de vraissemblance, pour l'échantillon {0},\\\n",
    "    obtient les estimateurs suivants: \\n\".format(ech))\n",
    "    print(\"mu0 = {0}\\n mu1 = {1} \\n sigma = \\n {2}\".format(mu0, mu1, sigma))\n",
    "\n",
    "    \n",
    "    pi = len(s1)/n #pi chapeau\n",
    "    pa = np.log(pi/(1-pi))\n",
    "    \n",
    "    print(\"pi = {0}\".format(pi))\n",
    "    \n",
    "    testes = np.zeros((n2, p2)) #On va essayer les prédictions sur l'échantillon de testes\n",
    "\n",
    "    constante = np.dot(mu1*(1/2), np.dot(invsig, mu1.transpose())) - np.dot(mu0*(1/2),\\\n",
    "                                    np.dot(invsig, mu0.transpose()))\n",
    "    mu = mu0-mu1\n",
    "\n",
    "    for i, elm in enumerate(dataTest):\n",
    "        coef = pa - np.dot(elm[0:2], np.dot(invsig, mu.transpose())) - constante\n",
    "\n",
    "        testes[i, 0:2] = elm[0:2]\n",
    "        testes[i, 2] = int(coef > 0)\n",
    "\n",
    "    errors = sum(testes[0:, 2] != dataTest[0:, 2])\n",
    "        #On calcule le taux d'erreurs\n",
    "    print(\"The error rate of the estimation on the testing set is {0}/{1} or : {2}%\"\\\n",
    "          .format(errors, n2, 100*errors/n2))\n",
    "    \n",
    "    #On prédit sur le train set pour calculer le taux d'erreur\n",
    "    pred = np.zeros((n, p))\n",
    "    for i, elm in enumerate(dataTrain):\n",
    "        coef = pa - np.dot(elm[0:2], np.dot(invsig, mu.transpose())) - constante\n",
    "\n",
    "        pred[i, 0:2] = elm[0:2]\n",
    "        pred[i, 2] = int(coef > 0)\n",
    "        \n",
    "\n",
    "    \n",
    "    errors2 = sum(pred[0:, 2] != dataTrain[0:, 2])\n",
    "    \n",
    "    print(\"The error rate of the estimation on the training set is {0}/{1} or : {2}%\"\\\n",
    "          .format(errors2, n, 100*errors2/n))\n",
    "    \n",
    "    #On plot les graphiques\n",
    "    x1 = np.linspace(-3, 3, 2) #Pour tracer la frontière\n",
    "    x2 = (pa - constante - x1*(mu[0]*invsig[0,0] + mu[1]*invsig[0,1])) \\\n",
    "                        /(((mu[0]*invsig[1,0]) + (mu[1]*invsig[1,1])))\n",
    "    \n",
    "    a = plt.scatter(dataTrain[dataTrain[0:,2] == 1][0:,0], \\\n",
    "                    dataTrain[dataTrain[0:,2] == 1][0:,1], color=\"red\", s=10)\n",
    "    b = plt.scatter(dataTrain[dataTrain[0:,2] == 0][0:,0], \\\n",
    "                    dataTrain[dataTrain[0:,2] == 0][0:,1], color=\"blue\", s=10)\n",
    "    c = plt.plot(x1, x2, color=\"orange\")\n",
    "\n",
    "    plt.title(\"Linear discriminant analysis for data {0} on training set \".format(ech))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend((a,b), ('Data with label 1','Data with label 0'), loc=\"upper right\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    a = plt.scatter(dataTest[dataTest[0:,2] == 1][0:,0], \\\n",
    "                    dataTest[dataTest[0:,2] == 1][0:,1], color=\"red\", s=10)\n",
    "    b = plt.scatter(dataTest[dataTest[0:,2] == 0][0:,0], \\\n",
    "                    dataTest[dataTest[0:,2] == 0][0:,1], color=\"blue\",s=10)\n",
    "    c = plt.plot(x1, x2, color=\"orange\")\n",
    "    \n",
    "    plt.title(\"Linear discriminant analysis for data {0} on testing set \".format(ech))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend((a,b), ('Data with label 1','Data with label 0'), loc=\"upper right\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par maximum de vraissemblance, pour l'échantillon A,    obtient les estimateurs suivants: \n",
      "\n",
      "mu0 = [ 2.89970947 -0.893874  ]\n",
      " mu1 = [-2.69232004  0.866042  ] \n",
      " sigma = \n",
      " [[ 2.44190897 -1.13194024]\n",
      " [-1.13194024  0.61375465]]\n",
      "pi = 0.3333333333333333\n",
      "The error rate of the estimation on the testing set is 30/1500 or : 2.0%\n",
      "The error rate of the estimation on the training set is 2/150 or : 1.3333333333333333%\n"
     ]
    }
   ],
   "source": [
    "LDA(dataTrain, dataTest, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par maximum de vraissemblance, pour l'échantillon B,    obtient les estimateurs suivants: \n",
      "\n",
      "mu0 = [ 3.34068896 -0.83546333]\n",
      " mu1 = [-3.21670734  1.08306733] \n",
      " sigma = \n",
      " [[ 3.34623467 -0.13516489]\n",
      " [-0.13516489  1.73807475]]\n",
      "pi = 0.5\n",
      "The error rate of the estimation on the testing set is 83/2000 or : 4.15%\n",
      "The error rate of the estimation on the training set is 9/300 or : 3.0%\n"
     ]
    }
   ],
   "source": [
    "LDA(dataTrainB, dataTestB, \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par maximum de vraissemblance, pour l'échantillon C,    obtient les estimateurs suivants: \n",
      "\n",
      "mu0 = [ 2.79304824 -0.83838667]\n",
      " mu1 = [-2.94232885 -0.9578284 ] \n",
      " sigma = \n",
      " [[ 2.88039225 -0.63405081]\n",
      " [-0.63405081  5.19952435]]\n",
      "pi = 0.625\n",
      "The error rate of the estimation on the testing set is 127/3000 or : 4.233333333333333%\n",
      "The error rate of the estimation on the training set is 22/400 or : 5.5%\n"
     ]
    }
   ],
   "source": [
    "LDA(dataTrainC, dataTestC, \"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(dataTrain, dataTest, ech):\n",
    "    n, p = dataTrain.shape\n",
    "    n2, p2 = dataTest.shape\n",
    "    \n",
    "    X = np.concatenate((np.ones((n, 1)), dataTrain[0:,0:2]), axis=1) #On rajoute la colonne de 1\n",
    "    Y = dataTrain[0:, 2]\n",
    "\n",
    "    def sigmoid(a): \n",
    "        return 1/(1+np.exp(-a))\n",
    "\n",
    "    def IRLS(X, Y): #On implémente l'algorithme IRLS pour déterminer les coefficients de la régression \n",
    "                    #MLE par descente de gradient\n",
    "        n, p = X.shape\n",
    "        w = np.zeros((p, 1))\n",
    "        eta = sigmoid(np.dot(X, w))\n",
    "        D = np.array(np.diag(list(eta.transpose())[0]))\n",
    "\n",
    "        wnext = w + np.dot(np.dot(np.linalg.inv(np.dot(X.transpose(), np.dot(D, X))), X.transpose()), Y.reshape(n,1) \\\n",
    "                           - eta)\n",
    "\n",
    "        while max(abs(wnext - w)) > 1e-3:\n",
    "            w = wnext\n",
    "            eta = sigmoid(np.dot(X, w))\n",
    "            D = np.array(np.diag(list(eta.transpose())[0]))\n",
    "            wnext = w + np.dot(np.dot(np.linalg.inv(np.dot(X.transpose(), np.dot(D, X))), X.transpose()), Y.reshape(n,1) \\\n",
    "                               - eta)\n",
    "        return wnext\n",
    "\n",
    "\n",
    "    W = IRLS(X, Y)\n",
    "\n",
    "    Xtilde = np.concatenate((np.ones((n2, 1)), dataTest[0:,0:2]), axis=1) #Echantillon de testes\n",
    "    Ytilde = dataTest[0:, 2]\n",
    "\n",
    "\n",
    "     #Paramètres appris:\n",
    "    print(\"Via l'algorithme IRLS, on obtient comme paramètres appris: \\n\")\n",
    "    print(\"W = {0}\".format(W))\n",
    "\n",
    "    predict = np.dot(W.transpose(), Xtilde.transpose())>0\n",
    "    #On prédit les sur l'échantillon test (si on la proba d'ếtre dans 1 est plus grande que 1/2 on place l'observation\n",
    "    #dans la classe 1, sinon elle va dans la classe 2\n",
    "\n",
    "\n",
    "    #On calcule le taux d'erreur\n",
    "    #Sur le testing set\n",
    "    error = np.sum(Ytilde != predict)\n",
    "    print(\"The error rate of the estimation on the testing set is {0}/{1} or {2}%\"\\\n",
    "          .format(error, n2, 100*error/n2)) \n",
    "    \n",
    "    #Sur le training set\n",
    "    predictTrain = np.dot(W.transpose(),X.transpose())>0\n",
    "    error2 = sum(Y != predictTrain[0])\n",
    "    print(\"The error rate of the estimation on the training set is {0}/{1} or {2}%\"\\\n",
    "    .format(error2, n, 100*error2/n))\n",
    "    \n",
    "    x1 = np.linspace(-5, 5, 2) #On trace la frontière\n",
    "    x2 = (1/W[2])*(-x1*W[1] - W[0])\n",
    "\n",
    "\n",
    "    a = plt.scatter(dataTrain[dataTrain[0:,2] == 1][0:,0],\\\n",
    "                    dataTrain[dataTrain[0:,2] == 1][0:,1], color=\"red\", s=10)\n",
    "    b = plt.scatter(dataTrain[dataTrain[0:,2] == 0][0:,0],\\\n",
    "                    dataTrain[dataTrain[0:,2] == 0][0:,1], color=\"blue\", s=10)\n",
    "    c = plt.plot(x1, x2, color=\"orange\")\n",
    "\n",
    "    plt.title(\"Logistic regression for data {0} on training set \".format(ech))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend((a,b), ('Data with label 1','Data with label 0'), loc=\"upper right\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    a = plt.scatter(dataTest[dataTest[0:,2] == 1][0:,0],\\\n",
    "                    dataTest[dataTest[0:,2] == 1][0:,1], color=\"red\", s=10)\n",
    "    b = plt.scatter(dataTest[dataTest[0:,2] == 0][0:,0],\\\n",
    "                    dataTest[dataTest[0:,2] == 0][0:,1], color=\"blue\", s=10)\n",
    "    c = plt.plot(x1, x2, color=\"orange\")\n",
    "\n",
    "    plt.title(\"Logistic regression for data {0} on testing set \".format(ech))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend((a,b), ('Data with label 1','Data with label 0'), loc=\"upper right\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Via l'algorithme IRLS, on obtient comme paramètres appris: \n",
      "\n",
      "W = [[ -4.70399596]\n",
      " [-24.80361334]\n",
      " [-42.13349832]]\n",
      "The error rate of the estimation on the testing set is 53/1500 or 3.533333333333333%\n",
      "The error rate of the estimation on the training set is 0/150 or 0.0%\n"
     ]
    }
   ],
   "source": [
    "logistic(dataTrain, dataTest, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Via l'algorithme IRLS, on obtient comme paramètres appris: \n",
      "\n",
      "W = [[ 1.03367852]\n",
      " [-1.49321724]\n",
      " [ 0.88179918]]\n",
      "The error rate of the estimation on the testing set is 83/2000 or 4.15%\n",
      "The error rate of the estimation on the training set is 6/300 or 2.0%\n"
     ]
    }
   ],
   "source": [
    "logistic(dataTrainB, dataTestB, \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Via l'algorithme IRLS, on obtient comme paramètres appris: \n",
      "\n",
      "W = [[ 0.65769293]\n",
      " [-1.90248234]\n",
      " [ 0.49839342]]\n",
      "The error rate of the estimation on the testing set is 73/3000 or 2.433333333333333%\n",
      "The error rate of the estimation on the training set is 16/400 or 4.0%\n"
     ]
    }
   ],
   "source": [
    "logistic(dataTrainC, dataTestC, \"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linearReg(dataTrain, dataTest, ech):    \n",
    "    n, p = dataTrain.shape\n",
    "    n2, p2 = dataTest.shape\n",
    "    \n",
    "    X = np.concatenate((np.ones((n, 1)), dataTrain[0:,0:2]), axis=1) #On rajoute la colonne de 1\n",
    "    Y = dataTrain[0:, 2]\n",
    "\n",
    "    B = np.dot(np.dot(np.linalg.inv(np.dot(X.transpose(), X)), X.transpose()), Y).reshape(3,1)\n",
    "    #On calcule beta chapeau par la formule qu'on connaît bien\n",
    "   \n",
    "    \n",
    "    print(\"Via la formule de l'estimateur de Beta: \\n\")\n",
    "    print(\"B = {0}\".format(B))\n",
    "\n",
    "    Xtilde = np.concatenate((np.ones((n2, 1)), dataTest[0:,0:2]), axis=1) #On ramène les échantillons de testes\n",
    "    Ytilde = dataTest[0:, 2]\n",
    "\n",
    "    predict = np.dot(Xtilde, B)>1/2 #On prédit les classes des testes\n",
    "    \n",
    "    #On calcule le taux d'erreurs\n",
    "    #Sur le testing set\n",
    "    error = np.sum(Ytilde != predict.transpose())\n",
    "    print(\"The error rate of the estimation on the testing set is {0}/{1} or {2}%\"\\\n",
    "          .format(error, n2, 100*error/n2))\n",
    "    \n",
    "    #Sur le training set\n",
    "    predictTrain = np.dot(X, B)>1/2\n",
    "    error2 = np.sum(Y != predictTrain.transpose())\n",
    "    print(\"The error rate of the estimation on the training set is {0}/{1} or {2}%\"\\\n",
    "          .format(error2, n, 100*error2/n))\n",
    "    \n",
    "    x1 = np.linspace(-5, 5, 2) #On calcule la frontière\n",
    "    x2 = (1/B[2])*((1/2) - B[0] - x1*B[1])\n",
    "\n",
    "\n",
    "\n",
    "    a = plt.scatter(dataTrain[dataTrain[0:,2] == 1][0:,0],\\\n",
    "                    dataTrain[dataTrain[0:,2] == 1][0:,1], color=\"red\", s=10)\n",
    "    b = plt.scatter(dataTrain[dataTrain[0:,2] == 0][0:,0],\\\n",
    "                    dataTrain[dataTrain[0:,2] == 0][0:,1], color=\"blue\", s=10)\n",
    "    c = plt.plot(x1, x2, color=\"orange\")\n",
    "\n",
    "    plt.title(\"Linear regression for data {0} on training set \".format(ech))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend((a,b), ('Data with label 1','Data with label 0'), loc=\"upper right\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    a = plt.scatter(dataTest[dataTest[0:,2] == 1][0:,0],\\\n",
    "                    dataTest[dataTest[0:,2] == 1][0:,1], color=\"red\", s=10)\n",
    "    b = plt.scatter(dataTest[dataTest[0:,2] == 0][0:,0], \\\n",
    "                    dataTest[dataTest[0:,2] == 0][0:,1], color=\"blue\", s=10)\n",
    "    c = plt.plot(x1, x2, color=\"orange\")\n",
    "\n",
    "    plt.title(\"Linear regression for data {0} on testing set \".format(ech))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend((a,b), ('Data with label 1','Data with label 0'), loc=\"upper right\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Via la formule de l'estimateur de Beta: \n",
      "\n",
      "B = [[ 0.49229204]\n",
      " [-0.2640075 ]\n",
      " [-0.37259311]]\n",
      "The error rate of the estimation on the testing set is 31/1500 or 2.066666666666667%\n",
      "The error rate of the estimation on the training set is 2/150 or 1.3333333333333333%\n"
     ]
    }
   ],
   "source": [
    "linearReg(dataTrain, dataTest, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Via la formule de l'estimateur de Beta: \n",
      "\n",
      "B = [[ 0.50005043]\n",
      " [-0.10424575]\n",
      " [ 0.05179118]]\n",
      "The error rate of the estimation on the testing set is 83/2000 or 4.15%\n",
      "The error rate of the estimation on the training set is 9/300 or 3.0%\n"
     ]
    }
   ],
   "source": [
    "linearReg(dataTrainB, dataTestB, \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Via la formule de l'estimateur de Beta: \n",
      "\n",
      "B = [[ 0.50839982]\n",
      " [-0.12769333]\n",
      " [-0.01700142]]\n",
      "The error rate of the estimation on the testing set is 127/3000 or 4.233333333333333%\n",
      "The error rate of the estimation on the training set is 22/400 or 5.5%\n"
     ]
    }
   ],
   "source": [
    "linearReg(dataTrainC, dataTestC, \"C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def QDA(dataTrain, dataTest, ech):\n",
    "    n, p = dataTrain.shape\n",
    "    n2, p2 = dataTest.shape\n",
    "    \n",
    "    mu1 = np.mean(dataTrain[dataTrain[0:,2] == 1][0:,0:2], axis=0) #mu1 chapeau\n",
    "    mu0 = np.mean(dataTrain[dataTrain[0:,2] == 0][0:,0:2], axis=0) #mu0 chapeau\n",
    "\n",
    "    s1 = dataTrain[dataTrain[0:,2] == 1][0:, 0:2] - mu1\n",
    "    sig1 = np.dot(s1.transpose(), s1)/len(s1)\n",
    "\n",
    "    s0 = dataTrain[dataTrain[0:,2] == 0][0:, 0:2] - mu0\n",
    "    sig0 = np.dot(s0.transpose(), s0)/len(s0)\n",
    "\n",
    "    invsig0 = np.linalg.inv(sig0)  #Inverse de sigma chapeau 0\n",
    "    invsig1 = np.linalg.inv(sig1)  #Inverse de sigma chapeau 1\n",
    "    \n",
    "    print(\"Par maximum de vraissemblance, pour l'échantillon {0},\\\n",
    "    obtient les estimateurs suivants: \\n\".format(ech))\n",
    "    print(\"mu0 = {0}\\n mu1 = {1} \\n sigma0 = \\n {2} \\n sigma1={3}\".format(mu0, mu1, sig0, sig1))\n",
    "\n",
    "    \n",
    "    pi = len(s1)/n #pi chapeau\n",
    "    pa = np.log(pi/(1-pi))\n",
    "\n",
    "    \n",
    "    print(\"pi = {0}\".format(pi))\n",
    "    \n",
    "    testes = np.zeros((n2, p2)) #On va essayer les prédictions sur l'échantillon de testes\n",
    "\n",
    "    constante = np.dot(mu1*(1/2), np.dot(invsig1, mu1.transpose())) - np.dot(mu0*(1/2),\\\n",
    "                                    np.dot(invsig0, mu0.transpose())) - pa + \\\n",
    "                                    (1/2)*np.log(np.linalg.det(sig1)/np.linalg.det(sig0))\n",
    "    \n",
    "    sigdifmu = np.dot(invsig0, mu0.transpose()) - np.dot(invsig1,mu1.transpose())\n",
    "    sigdif = invsig1 - invsig0\n",
    "    \n",
    "    print(sigdifmu)\n",
    "    \n",
    "    for i, elm in enumerate(dataTest):\n",
    "        coef = -np.dot(elm[0:2], sigdifmu.transpose()) - constante \\\n",
    "            -(1/2)*np.dot(np.dot(elm[0:2], sigdif), elm[0:2].transpose())\n",
    "        testes[i, 0:2] = elm[0:2]\n",
    "        testes[i, 2] = int(coef > 0)\n",
    "\n",
    "        #On calcule le taux d'erreurs\n",
    "    error = sum(testes[0:, 2] != dataTest[0:, 2])\n",
    "    print(\"The error rate of the estimation on the test set is {0}/{1} or {2}%\"\\\n",
    "          .format(error, n2, 100*error/n2))\n",
    "    \n",
    "    \n",
    "    #On prédit sur le train set pour calculer le taux d'erreur\n",
    "    pred = np.zeros((n, p))\n",
    "    for i, elm in enumerate(dataTrain):\n",
    "        coef = -np.dot(elm[0:2], sigdifmu.transpose()) - constante \\\n",
    "            -(1/2)*np.dot(np.dot(elm[0:2], sigdif), elm[0:2].transpose())\n",
    "\n",
    "        pred[i, 0:2] = elm[0:2]\n",
    "        pred[i, 2] = int(coef > 0)\n",
    "        \n",
    "    error2 = sum(pred[0:, 2] != dataTrain[0:, 2])\n",
    "    print(\"The error rate of the estimation on the training set is {0}/{1} or {2}%\"\\\n",
    "          .format(error2, n, 100*error2/n))\n",
    "    \n",
    "    #On plot les graphiques\n",
    "    \n",
    "    def b(x):\n",
    "        return (1/2)*x*sigdif[1,0] + (1/2)*x*sigdif[0,1] + sigdifmu[1]\n",
    "    \n",
    "    def c(x):\n",
    "        return (1/2)*sigdif[0,0]*x**2 + sigdifmu[0]*x + constante\n",
    "    \n",
    "    def delta(x):\n",
    "        return b(x)**2 - 2*sigdif[1,1]*c(x)\n",
    "    \n",
    "    x1 = np.linspace(-5, 10, 1000) #Pour tracer la frontière\n",
    "    \n",
    "    x1 = x1[delta(x1) >0]\n",
    "    \n",
    "    x2a = (-b(x1) + np.sqrt(delta(x1)))/sigdif[1,1]\n",
    "    x2b = (-b(x1) - np.sqrt(delta(x1)))/sigdif[1,1]\n",
    "    \n",
    "    print(min(x2b))\n",
    "    \n",
    "    a = plt.scatter(dataTrain[dataTrain[0:,2] == 1][0:,0],\\\n",
    "                    dataTrain[dataTrain[0:,2] == 1][0:,1], color=\"red\", s=10)\n",
    "    b = plt.scatter(dataTrain[dataTrain[0:,2] == 0][0:,0],\\\n",
    "                    dataTrain[dataTrain[0:,2] == 0][0:,1], color=\"blue\", s=10)\n",
    "    c = plt.plot(x1, x2a, color=\"orange\")\n",
    "    d = plt.plot(x1, x2b, color=\"orange\")\n",
    "    \n",
    "    \n",
    "    plt.title(\"Quadratic discriminant analysis for data {0} on training set \".format(ech))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend((a,b), ('Data with label 1','Data with label 0'), loc=\"upper right\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    a = plt.scatter(dataTest[dataTest[0:,2] == 1][0:,0],\\\n",
    "                    dataTest[dataTest[0:,2] == 1][0:,1], color=\"red\", s=10)\n",
    "    b = plt.scatter(dataTest[dataTest[0:,2] == 0][0:,0],\\\n",
    "                    dataTest[dataTest[0:,2] == 0][0:,1], color=\"blue\", s=10)\n",
    "    c = plt.plot(x1, x2a, color=\"orange\")\n",
    "    d = plt.plot(x1, x2b, color=\"orange\")\n",
    "    \n",
    "    plt.title(\"Quadratic discriminant analysis for data {0} on testing set \".format(ech))\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.legend((a,b), ('Data with label 1','Data with label 0'), loc=\"upper right\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par maximum de vraissemblance, pour l'échantillon A,    obtient les estimateurs suivants: \n",
      "\n",
      "mu0 = [ 2.89970947 -0.893874  ]\n",
      " mu1 = [-2.69232004  0.866042  ] \n",
      " sigma0 = \n",
      " [[ 2.31065259 -1.04748461]\n",
      " [-1.04748461  0.57578403]] \n",
      " sigma1=[[ 2.70442172 -1.3008515 ]\n",
      " [-1.3008515   0.68969588]]\n",
      "pi = 0.3333333333333333\n",
      "[  7.36527314  10.87335416]\n",
      "The error rate of the estimation on the test set is 30/1500 or 2.0%\n",
      "The error rate of the estimation on the training set is 1/150 or 0.6666666666666666%\n",
      "-6.00708203095\n"
     ]
    }
   ],
   "source": [
    "QDA(dataTrain, dataTest, 'A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par maximum de vraissemblance, pour l'échantillon B,    obtient les estimateurs suivants: \n",
      "\n",
      "mu0 = [ 3.34068896 -0.83546333]\n",
      " mu1 = [-3.21670734  1.08306733] \n",
      " sigma0 = \n",
      " [[ 2.53885859  1.0642112 ]\n",
      " [ 1.0642112   2.96007891]] \n",
      " sigma1=[[ 4.15361075 -1.33454097]\n",
      " [-1.33454097  0.51607059]]\n",
      "pi = 0.5\n",
      "[ 2.28065009 -1.45700199]\n",
      "The error rate of the estimation on the test set is 40/2000 or 2.0%\n",
      "The error rate of the estimation on the training set is 4/300 or 1.3333333333333333%\n",
      "-0.664046894487\n"
     ]
    }
   ],
   "source": [
    "QDA(dataTrainB, dataTestB, 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par maximum de vraissemblance, pour l'échantillon C,    obtient les estimateurs suivants: \n",
      "\n",
      "mu0 = [ 2.79304824 -0.83838667]\n",
      " mu1 = [-2.94232885 -0.9578284 ] \n",
      " sigma0 = \n",
      " [[ 2.89913927  1.24581553]\n",
      " [ 1.24581553  2.92475448]] \n",
      " sigma1=[[ 2.86914403 -1.76197061]\n",
      " [-1.76197061  6.56438626]]\n",
      "pi = 0.625\n",
      "[ 2.66524064 -0.34888942]\n",
      "The error rate of the estimation on the test set is 115/3000 or 3.8333333333333335%\n",
      "The error rate of the estimation on the training set is 21/400 or 5.25%\n",
      "-1.20513481548\n"
     ]
    }
   ],
   "source": [
    "QDA(dataTrainC, dataTestC, 'C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
